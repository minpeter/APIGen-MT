from enum import Enum
from pydantic import BaseModel, Field, ValidationError, field_validator
from typing import List, Dict, Any, Optional
import json
import re
import copy # For deep copying objects
import os
from transformers import AutoTokenizer
from llm_client import LLMClient
from tool_manager import ToolManager

# --- Pydantic Models for APIGen-MT Phase 1 (largely unchanged) ---
class ToolCallInternal(BaseModel):
    tool_name: str = Field(..., description="í˜¸ì¶œí•  ë„êµ¬ì˜ ì´ë¦„")
    arguments: Dict[str, Any] = Field(default_factory=dict, description="ë„êµ¬ í˜¸ì¶œì— í•„ìš”í•œ ì¸ìë“¤ (ë”•ì…”ë„ˆë¦¬ í˜•íƒœ)")

class AGTStep(BaseModel):
    tool_calls: List[ToolCallInternal] = Field(..., description="Ground Truth: ì´ ë‹¨ê³„ì—ì„œ ì—ì´ì „íŠ¸ê°€ ìˆ˜í–‰í•´ì•¼ í•  ë„êµ¬ í˜¸ì¶œ ë¦¬ìŠ¤íŠ¸")

class Blueprint(BaseModel):
    q: str = Field(..., description="ì‚¬ìš©ìì˜ ì´ˆê¸° ì§ˆë¬¸ ë˜ëŠ” ìš”ì²­")
    a_gt_steps: List[AGTStep] = Field(..., description="Ground Truth Steps: ì—ì´ì „íŠ¸ê°€ ìˆ˜í–‰í•´ì•¼ í•  ë„êµ¬ í˜¸ì¶œ ë‹¨ê³„ ë¦¬ìŠ¤íŠ¸.")
    o_gt: str = Field(..., description="ì˜ˆìƒë˜ëŠ” ìµœì¢… ê²°ê³¼ì— ëŒ€í•œ ì„¤ëª… (ë¬¸ìì—´)")

    @field_validator('a_gt_steps')
    def check_a_gt_steps_not_empty(cls, v):
        if not v:
            raise ValueError('a_gt_stepsëŠ” ìµœì†Œ í•˜ë‚˜ ì´ìƒì˜ ë‹¨ê³„ë¥¼ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.')
        for step in v:
            if not step.tool_calls:
                raise ValueError('a_gt_stepsì˜ ê° ë‹¨ê³„ëŠ” ìµœì†Œ í•˜ë‚˜ ì´ìƒì˜ ë„êµ¬ í˜¸ì¶œì„ í¬í•¨í•´ì•¼ í•©ë‹ˆë‹¤.')
        return v

class BlueprintCandidate(BaseModel):
    blueprint: Blueprint
    generation_reasoning: Optional[str] = None

class ValidationResult(BaseModel):
    is_valid_format: bool = False
    format_errors: Optional[List[str]] = None
    is_executable: bool = False
    executability_checks: List[Dict[str, Any]] = Field(default_factory=list)
    overall_validation_passed: bool = False

class QualityEnum(str, Enum):
    Excellent = "Excellent"
    Good = "Good"
    Fair = "Fair"
    Poor = "Poor"

class LLMReview(BaseModel):
    quality_assessment: QualityEnum = Field(..., description="LLM ë¦¬ë·°ì˜ í’ˆì§ˆ í‰ê°€ (Excellent, Good, Fair, Poor ì¤‘ í•˜ë‚˜)")
    feedback_summary: Optional[str] = None
    suggested_corrections: Optional[str] = None

class VerifiedBlueprint(BaseModel):
    blueprint: Blueprint
    validation_result: ValidationResult
    llm_review_history: List[LLMReview] = Field(default_factory=list)
    generation_attempts: int

# --- APIGen-MT Phase 1 Generator (Adapted) ---
class APIGenMTPhase1Generator:
    def __init__(self, llm_client: LLMClient, tool_manager: ToolManager):
        self.llm = llm_client
        self.tool_manager = tool_manager

    def _process_placeholders(self, arguments: Dict[str, Any], execution_context: Dict[str, Any]) -> Dict[str, Any]:
        """
        Processes placeholders in arguments using execution context from previous steps.
        Moved from the old ToolManager into this class.
        """
        processed_args = copy.deepcopy(arguments)
        for arg_name, arg_value in processed_args.items():
            if isinstance(arg_value, str):
                placeholders = re.findall(r"\{\{([^{}]+)\}\}", arg_value)
                for placeholder_full_key in placeholders:
                    keys = placeholder_full_key.split('.')
                    current_value = execution_context
                    found = True
                    for key_part in keys:
                        if isinstance(current_value, dict) and key_part in current_value:
                            current_value = current_value[key_part]
                        else:
                            # print(f"Warning: Placeholder key part '{key_part}' not found in context for '{placeholder_full_key}'")
                            found = False
                            break
                    
                    if found:
                        placeholder_tag = "{{" + placeholder_full_key + "}}"
                        if arg_value == placeholder_tag:
                            processed_args[arg_name] = current_value
                        else:
                            processed_args[arg_name] = arg_value.replace(placeholder_tag, str(current_value))
                    # else:
                        # print(f"Warning: Could not resolve placeholder '{placeholder_full_key}' for argument '{arg_name}'. Kept as is.")
        return processed_args

    def _generate_blueprint_candidate(self, q: str, previous_feedback: Optional[str] = None, previous_llm_reviews: Optional[List[LLMReview]] = None, previous_validation_result: Optional[ValidationResult] = None) -> Optional[BlueprintCandidate]:
        tool_schemas_list = self.tool_manager.get_tools_json_schema()
        tool_schema_json_str = json.dumps(tool_schemas_list, indent=2, ensure_ascii=False)
        
        llm_review_prompt = ""
        if previous_llm_reviews:
            llm_review_prompt += "ì´ì „ LLM ë¦¬ë·° ë‚´ì—­ì´ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ì˜ í’ˆì§ˆ í‰ê°€ì™€ í”¼ë“œë°±ì„ ì°¸ê³ í•˜ì—¬ ì²­ì‚¬ì§„ì„ ê°œì„ í•´ì£¼ì‹­ì‹œì˜¤.\n"
            for idx, review in enumerate(previous_llm_reviews, 1):
                llm_review_prompt += f"ë¦¬ë·° {idx}:\n"
                llm_review_prompt += f"- í’ˆì§ˆ í‰ê°€: {review.quality_assessment}\n"
                if review.feedback_summary:
                    llm_review_prompt += f"- í”¼ë“œë°± ìš”ì•½: {review.feedback_summary}\n"
                if review.suggested_corrections:
                    llm_review_prompt += f"- ìˆ˜ì • ì œì•ˆ: {review.suggested_corrections}\n"
                llm_review_prompt += "\n"

        validation_prompt = ""
        if previous_validation_result and not previous_validation_result.overall_validation_passed:
            validation_prompt += "ì´ì „ ìë™ ê²€ì¦(validation) ê²°ê³¼ê°€ ì‹¤íŒ¨í•˜ì˜€ìŠµë‹ˆë‹¤. ì•„ë˜ì˜ ê²€ì¦ ê²°ê³¼ë¥¼ ì°¸ê³ í•˜ì—¬ ì²­ì‚¬ì§„ì„ ê°œì„ í•´ì£¼ì‹­ì‹œì˜¤.\n"
            if previous_validation_result.format_errors:
                validation_prompt += f"- í˜•ì‹ ì˜¤ë¥˜: {'; '.join(previous_validation_result.format_errors)}\n"
            if not previous_validation_result.is_executable:
                failed_checks = [
                    f"Step {chk['step_index']}-Tool {chk['tool_name']}: {chk['reason']}"
                    for chk in previous_validation_result.executability_checks if not chk.get("can_execute")
                ]
                if failed_checks:
                    validation_prompt += f"- ì‹¤í–‰ ë¶ˆê°€ ì‚¬ìœ : {'; '.join(failed_checks)}\n"
            validation_prompt += "\n"

        system_prompt = f"""ë‹¹ì‹ ì€ APIGen-MT íŒŒì´í”„ë¼ì¸ì˜ 1ë‹¨ê³„ë¥¼ ìœ„í•œ 'ì‘ì—… ì²­ì‚¬ì§„ ìƒì„±ì'ì…ë‹ˆë‹¤. í˜„ì‹¤ì ì´ê³  ê²€ì¦ ê°€ëŠ¥í•œ ë‹¤ì¤‘ í„´ ìƒí˜¸ì‘ìš© ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ìƒì„¸í•œ ì²­ì‚¬ì§„ì„ JSON ê°ì²´ë¡œ ìƒì„±í•´ì•¼ í•©ë‹ˆë‹¤.

ì£¼ìš” ëª©í‘œ:
1.  **ë‹¤ì¤‘ ë‹¨ê³„ ë° ì˜ì¡´ì„±**: ì‚¬ìš©ìì˜ ìš”ì²­(`q`)ì„ í•´ê²°í•˜ê¸° ìœ„í•´ ì—¬ëŸ¬ ë‹¨ê³„ì˜ ë„êµ¬ í˜¸ì¶œì´ í•„ìš”í•˜ê³ , í•œ ë„êµ¬ì˜ ì¶œë ¥ì´ ë‹¤ìŒ ë„êµ¬ì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©ë˜ëŠ” ì‹œë‚˜ë¦¬ì˜¤ë¥¼ ìš°ì„ ì ìœ¼ë¡œ ìƒì„±í•©ë‹ˆë‹¤.
2.  **ë³‘ë ¬ ì‹¤í–‰ ê°€ëŠ¥ì„±**: `a_gt_steps`ì˜ ê° ë‹¨ê³„(`AGTStep`) ë‚´ `tool_calls` ë¦¬ìŠ¤íŠ¸ì—ëŠ” í•´ë‹¹ ë‹¨ê³„ì—ì„œ ë³‘ë ¬ë¡œ ì‹¤í–‰ë  ìˆ˜ ìˆëŠ” ë„êµ¬ë“¤ì„ í¬í•¨ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì˜ˆ: ì„œë¡œ ë…ë¦½ì ì¸ ì •ë³´ ì¡°íšŒ)
3.  **ì •í™•í•œ ë„êµ¬ ì‚¬ìš©**: ì œê³µëœ ë„êµ¬ ì„¤ëª…(`Available Tools and their Schemas`)ì„ ì •í™•íˆ ì°¸ì¡°í•˜ì—¬ `tool_name`ê³¼ `arguments`ë¥¼ êµ¬ì„±í•´ì•¼ í•©ë‹ˆë‹¤. ì¸ì ì´ë¦„ê³¼ íƒ€ì…ì´ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.
4.  **í”Œë ˆì´ìŠ¤í™€ë”**: ì´ì „ ë‹¨ê³„ ë„êµ¬ì˜ ì¶œë ¥ì„ ì°¸ì¡°í•  ë•ŒëŠ” `{{tool_name.output.field_name}}` í˜•ì‹ì˜ í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì‚¬ìš©í•˜ì‹­ì‹œì˜¤. (ì˜ˆ: `{{search_event.output.date}}`) `execution_context`ì—ì„œ í•´ë‹¹ ê²½ë¡œë¡œ ê°’ì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ ì •í™•íˆ ëª…ì‹œí•´ì•¼ í•©ë‹ˆë‹¤.
5.  **ìì—°ìŠ¤ëŸ¬ìš´ `o_gt`**: ëª¨ë“  `a_gt_steps`ê°€ ì„±ê³µì ìœ¼ë¡œ ì‹¤í–‰ë˜ì—ˆë‹¤ê³  ê°€ì •í–ˆì„ ë•Œ, ì—ì´ì „íŠ¸ê°€ ì‚¬ìš©ìì—ê²Œ ì œê³µí•  ìµœì¢… ì‘ë‹µ(`o_gt`)ì€ ìì—°ìŠ¤ëŸ½ê³ , ëª¨ë“  ì£¼ìš” ì‘ì—… ê²°ê³¼ë¥¼ ìš”ì•½í•´ì•¼ í•©ë‹ˆë‹¤.

{llm_review_prompt}{validation_prompt}
ì¶œë ¥ JSON í˜•ì‹ (Pydantic ëª¨ë¸ `Blueprint` ì¤€ìˆ˜):
```json
{{
  "q": "ì‚¬ìš©ì ìš”ì²­ ë¬¸ìì—´",
  "a_gt_steps": [
    {{"tool_calls": [{{"tool_name": "ë„êµ¬ëª…1", "arguments": {{"ì¸ìëª…": "ê°’"}}}}]}},
    {{"tool_calls": [{{"tool_name": "ë„êµ¬ëª…2", "arguments": {{"ì¸ìëª…": "{{ë„êµ¬ëª…1.output.í•„ë“œëª…}}"}}}}]}}
  ],
  "o_gt": "ìµœì¢… ê²°ê³¼ì— ëŒ€í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì„¤ëª…"
}}
```
**ì‚¬ê³  ê³¼ì • (ìƒëµ ê°€ëŠ¥, ê¶Œì¥):**
ìµœì¢… JSONì„ ìƒì„±í•˜ê¸° ì „ì—, ë‹¹ì‹ ì˜ ì‚¬ê³  ê³¼ì • (ì–´ë–¤ ì‹œë‚˜ë¦¬ì˜¤ë¥¼ êµ¬ìƒí–ˆê³ , ì™œ íŠ¹ì • ë„êµ¬ì™€ ìˆœì„œ, ë³‘ë ¬ì„±ì„ ì„ íƒí–ˆëŠ”ì§€ ë“±)ì„ `<think>...</think>` íƒœê·¸ ì•ˆì— ì‘ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

Available Tools and their Schemas:
{tool_schema_json_str}
"""
        user_prompt = f"User Query (q): {q}\n\n"
        if previous_feedback:
            user_prompt += f"ì´ì „ ì‹œë„ì— ëŒ€í•œ í”¼ë“œë°±ì…ë‹ˆë‹¤. ì´ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ì²­ì‚¬ì§„ì„ ê°œì„ í•´ì£¼ì‹­ì‹œì˜¤:\n{previous_feedback}\n\n"
        user_prompt += "ìœ„ ê°€ì´ë“œë¼ì¸ê³¼ ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬ë¥¼ ì°¸ê³ í•˜ì—¬, ì´ ìš”ì²­ì— ëŒ€í•œ Blueprint JSONì„ ìƒì„±í•´ì£¼ì‹­ì‹œì˜¤."

        try:
            parsed_json, reasoning = self.llm.json_output(
                prompt=user_prompt,
                system_prompt=system_prompt, # This detailed system prompt will be used by LLMClient
                schema=Blueprint,
                reasoning=True
            )
            if parsed_json:
                blueprint_obj = Blueprint(**parsed_json)

                print(f"Debug: Generated Blueprint JSON: {json.dumps(parsed_json, indent=2, ensure_ascii=False)}")

                return BlueprintCandidate(blueprint=blueprint_obj, generation_reasoning=reasoning)
            else:
                print(f"Error: LLM did not return valid JSON for q: {q}")
                return None
        except ValidationError as e:
            print(f"Pydantic validation error for blueprint from LLM for q: {q}: {e}")
            return None
        except Exception as e:
            print(f"Unexpected error during blueprint generation for q: {q}: {e}")
            return None

    def _validate_blueprint_format_and_executability(self, blueprint: Blueprint) -> ValidationResult:
        format_errors: List[str] = [] # Should be caught by Pydantic mostly
        executability_checks: List[Dict[str, Any]] = []
        is_overall_executable = True
        current_execution_context: Dict[str, Any] = {}

        for step_idx, agt_step in enumerate(blueprint.a_gt_steps):
            step_executable = True
            step_results_for_context: Dict[str, Any] = {}

            for tool_idx, tool_call in enumerate(agt_step.tool_calls):
                check_result = {
                    "step_index": step_idx,
                    "tool_index_in_step": tool_idx,
                    "tool_name": tool_call.tool_name,
                    "arguments_original": copy.deepcopy(tool_call.arguments),
                    "arguments_processed": {},
                    "can_execute": False,
                    "reason": "",
                    "simulated_output": None
                }

                if not self.tool_manager.tool_exists(tool_call.tool_name):
                    check_result["reason"] = f"Tool '{tool_call.tool_name}' not found."
                    step_executable = False
                    executability_checks.append(check_result)
                    continue

                tool_schema = self.tool_manager.get_tool_schema(tool_call.tool_name)
                if not tool_schema: # Should not happen if tool_exists is true
                    check_result["reason"] = f"Schema not found for tool '{tool_call.tool_name}'."
                    step_executable = False
                    executability_checks.append(check_result)
                    continue
                
                try:
                    processed_args = self._process_placeholders(
                        tool_call.arguments, current_execution_context
                    )
                    check_result["arguments_processed"] = processed_args

                    # Validate required arguments after placeholder processing
                    for req_arg in tool_schema.get("parameters", {}).get("required", []):
                        if req_arg not in processed_args:
                            # If still missing, it's an error (placeholder didn't resolve or wasn't there)
                            raise ValueError(f"Missing required argument '{req_arg}' after placeholder processing.")

                    # Simulate tool invocation using the ToolManager
                    # The new ToolManager's invoke_tool uses an LLM to simulate.
                    tool_output = self.tool_manager.invoke_tool(
                        tool_call.tool_name,
                        processed_args # Pass processed arguments
                    )
                    check_result["can_execute"] = True
                    check_result["reason"] = "Successfully simulated invocation."
                    check_result["simulated_output"] = tool_output
                    
                    # Store output for context. Key by tool_name.output for clarity.
                    # Example: {"search_event.output": {"date": "2025-10-15", ...}}
                    # This structure helps in _process_placeholders if it expects such keys.
                    # The current _process_placeholders expects keys like "tool_name.output.field_name"
                    # so we should store the output directly under "tool_name.output"
                    if isinstance(tool_output, dict): # Assuming tool outputs are dicts
                         step_results_for_context[f"{tool_call.tool_name}.output"] = tool_output
                    else: # Simple string or other type
                         step_results_for_context[f"{tool_call.tool_name}.output"] = {"value": tool_output}


                except Exception as e:
                    check_result["reason"] = f"Execution/Validation failed: {str(e)}"
                    step_executable = False
                
                executability_checks.append(check_result)
            
            if not step_executable:
                is_overall_executable = False
                break 
            else:
                current_execution_context.update(step_results_for_context)

        is_valid_format = not bool(format_errors)
        overall_passed = is_valid_format and is_overall_executable

        return ValidationResult(
            is_valid_format=is_valid_format,
            format_errors=format_errors if format_errors else None,
            is_executable=is_overall_executable,
            executability_checks=executability_checks,
            overall_validation_passed=overall_passed
        )

    def _get_llm_review_and_feedback(self, blueprint_candidate: BlueprintCandidate, validation_result: ValidationResult) -> Optional[LLMReview]:
        blueprint_str = blueprint_candidate.blueprint.model_dump_json(indent=2)
        validation_str = validation_result.model_dump_json(indent=2)
        reasoning_str = blueprint_candidate.generation_reasoning or "ì œê³µë˜ì§€ ì•ŠìŒ."

        system_prompt = """ë‹¹ì‹ ì€ AI ì—ì´ì „íŠ¸ ê°œë°œì„ ìœ„í•œ ë°ì´í„° í’ˆì§ˆ ê´€ë¦¬ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì œê³µëœ ì‘ì—… ì²­ì‚¬ì§„(blueprint), ìƒì„± ì‹œ LLMì˜ ì‚¬ê³  ê³¼ì •(reasoning), ê·¸ë¦¬ê³  ìë™ ê²€ì¦ ê²°ê³¼(validation_result)ë¥¼ ë©´ë°€íˆ ê²€í† í•´ì£¼ì‹­ì‹œì˜¤. 
ë‹¹ì‹ ì˜ ëª©í‘œëŠ” ì´ ì²­ì‚¬ì§„ì´ ê³ í’ˆì§ˆ í•™ìŠµ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ë° ì í•©í•œì§€ í‰ê°€í•˜ê³ , ê°œì„ ì„ ìœ„í•œ êµ¬ì²´ì ì´ê³  ì‹¤í–‰ ê°€ëŠ¥í•œ í”¼ë“œë°±ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.

â€» ì°¸ê³ : í”Œë ˆì´ìŠ¤í™€ë”(`{{tool_name.output.field_name}}`)ì˜ ì‚¬ìš©ì€ ë‹¤ì¤‘ ë‹¨ê³„ ë° ì˜ì¡´ì„± êµ¬í˜„ì„ ìœ„í•œ í•„ìˆ˜ì ì¸ ìš”ì†Œì…ë‹ˆë‹¤. 
í”Œë ˆì´ìŠ¤í™€ë”ê°€ ê·œì¹™ì— ë§ê²Œ ì‚¬ìš©ë˜ì—ˆë‹¤ë©´, ì´ì— ëŒ€í•´ í˜ë„í‹°ë¥¼ ë¶€ì—¬í•˜ì§€ ë§ˆì‹­ì‹œì˜¤. 
ì˜¤íˆë ¤ í”Œë ˆì´ìŠ¤í™€ë”ë¥¼ ì˜¬ë°”ë¥´ê²Œ ì‚¬ìš©í•œ ê²½ìš°ëŠ” ê¸ì •ì ìœ¼ë¡œ í‰ê°€í•´ë„ ë©ë‹ˆë‹¤.

ë˜í•œ, ì•„ë˜ì˜ í”„ë¡¬í”„íŠ¸ ê·œì¹™ì„ ì¤€ìˆ˜í•˜ì˜€ë‹¤ë©´ ê³ í’ˆì§ˆ ë°ì´í„°ë¼ê³  í‰ê°€í•´ë„ ê´œì°®ìŠµë‹ˆë‹¤.
"""
        user_prompt = f"""ë‹¤ìŒì€ ê²€í† í•  ì‘ì—… ì²­ì‚¬ì§„ ë° ê´€ë ¨ ì •ë³´ì…ë‹ˆë‹¤:

1.  **ì‚¬ìš©ì ì´ˆê¸° ìš”ì²­ (q)**:
    ```
    {blueprint_candidate.blueprint.q}
    ```

2.  **ìƒì„±ëœ ì‘ì—… ì²­ì‚¬ì§„ (a_gt_steps, o_gt)**:
    ```json
    {blueprint_str}
    ```

3.  **ì²­ì‚¬ì§„ ìƒì„± ì‹œ LLMì˜ ì‚¬ê³  ê³¼ì •**:
    ```
    {reasoning_str}
    ```

4.  **ìë™ ê²€ì¦ ê²°ê³¼ (í˜•ì‹ ë° ì‹¤í–‰ ê°€ëŠ¥ì„±)**:
    ```json
    {validation_str}
    ```

**ê²€í†  í•­ëª© ë° í‰ê°€ ê¸°ì¤€:**
* **ìš”ì²­(q)ì˜ ëª…í™•ì„± ë° í˜„ì‹¤ì„±**
* **ë…¼ë¦¬ì  ì¼ê´€ì„± ë° ì •í™•ì„± (a_gt_steps)** (ìˆœì„œ, ë³‘ë ¬ì„±, í”Œë ˆì´ìŠ¤í™€ë” ì‚¬ìš©)
* **ë„êµ¬ ì‚¬ìš©ì˜ ì ì ˆì„±** (ì„ íƒëœ ë„êµ¬, ì¸ì ê°’)
* **ê²°ê³¼(o_gt)ì˜ ì ì ˆì„±**
* **ìë™ ê²€ì¦ ê²°ê³¼ì˜ ì‹œì‚¬ì **
* **ì „ë°˜ì ì¸ í’ˆì§ˆ**: (Excellent, Good, Fair, Poor ì¤‘ ì„ íƒ)

**ì¶œë ¥ í˜•ì‹:**
Pydantic ëª¨ë¸ `LLMReview`ì˜ JSON í˜•ì‹ì„ ë”°ë¼ì£¼ì‹­ì‹œì˜¤. `quality_assessment`ëŠ” í•„ìˆ˜ì´ë©°, "Poor" ë˜ëŠ” "Fair"ì¸ ê²½ìš° `feedback_summary`ì™€ `suggested_corrections`ë¥¼ êµ¬ì²´ì ìœ¼ë¡œ ì‘ì„±í•´ì£¼ì‹­ì‹œì˜¤.
        ```json
{{
  "quality_assessment": "...",
  "feedback_summary": "...",
  "suggested_corrections": "..."
}}
```
"""
        try:
            # LLMClient.chat returns (response_text, reasoning_text)
            # We expect response_text to be the JSON string for LLMReview
            response_obj, reasoning_text = self.llm.json_output(
                                                    system_prompt=system_prompt,
                                                    prompt=user_prompt,
                                                    schema=LLMReview
                                                  )

            # print(f"LLM Review Reasoning: {reasoning_text}")
            print(f"LLM Review Response: {response_obj}")
            
            # Convert the dictionary to a LLMReview object
            llm_review = LLMReview(**response_obj)
            return llm_review
        except Exception as e:
            print(f"Unexpected error during LLM review for q: {blueprint_candidate.blueprint.q}: {e}")
            return None

    def generate_verified_blueprint(self, q: str, max_attempts: int = 3) -> Optional[VerifiedBlueprint]:
        if not q.strip():
            print("Warning: Empty query provided. Skipping.")
            return None

        print(f"\n--- Starting Phase 1 for Query: \"{q}\" ---")
        
        current_blueprint_candidate: Optional[BlueprintCandidate] = None
        validation_result: Optional[ValidationResult] = None
        llm_review_history: List[LLMReview] = []
        last_feedback: Optional[str] = None
        previous_llm_reviews: List[LLMReview] = []

        for attempt in range(1, max_attempts + 1):
            print(f"\nAttempt {attempt}/{max_attempts}...")

            print("Step 1: Generating blueprint candidate...")
            current_blueprint_candidate = self._generate_blueprint_candidate(
                q,
                previous_feedback=last_feedback,
                previous_llm_reviews=previous_llm_reviews if previous_llm_reviews else None,
                previous_validation_result=validation_result if validation_result and not validation_result.overall_validation_passed else None
            )
            if not current_blueprint_candidate:
                print("Failed to generate blueprint candidate.")
                if attempt == max_attempts: break
                last_feedback = "LLMì´ ìœ íš¨í•œ ì²­ì‚¬ì§„ì„ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. í”„ë¡¬í”„íŠ¸ë¥¼ í™•ì¸í•˜ê³  ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”."
                continue
            print(f"Blueprint candidate generated for q: {current_blueprint_candidate.blueprint.q}")

            print("Step 2: Validating blueprint format and executability...")
            validation_result = self._validate_blueprint_format_and_executability(current_blueprint_candidate.blueprint)
            
            if not validation_result.overall_validation_passed:
                print("Blueprint validation failed.")
                feedback_parts = []
                if not validation_result.is_valid_format and validation_result.format_errors:
                    feedback_parts.append(f"Format errors: {'; '.join(validation_result.format_errors)}")
                if not validation_result.is_executable:
                    failed_checks_str = "; ".join([
                        f"Step {chk['step_index']}-Tool {chk['tool_name']}: {chk['reason']}"
                        for chk in validation_result.executability_checks if not chk.get("can_execute")
                    ])
                    feedback_parts.append(f"Executability issues: {failed_checks_str}")
                last_feedback = "ìë™ ê²€ì¦ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. " + " ".join(feedback_parts) + " ì´ ë¬¸ì œë¥¼ í•´ê²°í•˜ì—¬ ì²­ì‚¬ì§„ì„ ìˆ˜ì •í•´ì£¼ì„¸ìš”."
                if attempt == max_attempts: break
                continue
            print("Blueprint validation successful.")

            print("Step 3: Getting LLM review...")
            llm_review = self._get_llm_review_and_feedback(current_blueprint_candidate, validation_result)
            if not llm_review: # Could be None if LLM fails or returns unparsable review
                print("Failed to get LLM review or review was unparsable. Assuming 'Fair' quality and requesting refinement.")
                last_feedback = "LLM ë¦¬ë·°ë¥¼ ë°›ëŠ”ë° ì‹¤íŒ¨í–ˆê±°ë‚˜ ë¦¬ë·° í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ì²­ì‚¬ì§„ì„ ë‹¤ì‹œ í•œë²ˆ ê²€í† í•˜ê³  ê°œì„ í•´ì£¼ì„¸ìš”."
                # Optionally add a placeholder review to history
                llm_review_history.append(LLMReview(quality_assessment="ReviewFailed", feedback_summary=last_feedback))
                previous_llm_reviews = llm_review_history.copy()
                if attempt == max_attempts: break
                continue
            
            llm_review_history.append(llm_review)
            previous_llm_reviews = llm_review_history.copy()
            print(f"LLM Review Quality: {llm_review.quality_assessment}")

            if llm_review.quality_assessment in [QualityEnum.Excellent, QualityEnum.Good]:
                print("Blueprint approved by LLM review.")
                return VerifiedBlueprint(
                    blueprint=current_blueprint_candidate.blueprint,
                    validation_result=validation_result,
                    llm_review_history=llm_review_history,
                    generation_attempts=attempt
                )
            else:
                print("Blueprint requires refinement based on LLM review.")
                last_feedback = f"LLM ë¦¬ë·° ê²°ê³¼ '{llm_review.quality_assessment}'ì…ë‹ˆë‹¤. ë‹¤ìŒ í”¼ë“œë°±ì„ ë°˜ì˜í•˜ì—¬ ì²­ì‚¬ì§„ì„ ê°œì„ í•´ì£¼ì‹­ì‹œì˜¤: {llm_review.feedback_summary or ''} {llm_review.suggested_corrections or ''}"
                if attempt == max_attempts: break
                continue
        
        print(f"--- Failed to generate a verified blueprint for Query: \"{q}\" after {max_attempts} attempts. ---")
        return None

if __name__ == "__main__":
    print("Initializing APIGen-MT Phase 1 Generator components...")
    
    # Ensure FRIENDLI_TOKEN is set in your environment
    if not os.getenv("FRIENDLI_TOKEN"):
        print("ERROR: FRIENDLI_TOKEN environment variable not set. Exiting.")
        exit()

    # Use the actual LLMClient and ToolManager
    # You might want to configure api_model and hf_tokenizer_id based on your Friendli setup
    llm_client_instance = LLMClient(
        # url="YOUR_FRIENDLI_ENDPOINT_IF_DIFFERENT", # Optional
        # api_model="YOUR_FRIENDLI_MODEL_SLUG", # e.g., "meta-llama/Llama-2-7b-chat-hf"
        # hf_tokenizer_id="HF_TOKENIZER_FOR_YOUR_MODEL" # e.g., "meta-llama/Llama-2-7b-chat-hf"
    )
    tool_manager_instance = ToolManager(llm=llm_client_instance)

    phase1_generator = APIGenMTPhase1Generator(
        llm_client=llm_client_instance,
        tool_manager=tool_manager_instance
    )
    print("APIGen-MT Phase 1 Generator initialized with actual components.")

    # Test query
    query = "ADEX 2025 ì¼ì •ì„ ì¡°ì‚¬í•œ í›„, í–‰ì‚¬ ì¼ì •ì— ë§ì¶°ì„œ ìº˜ë¦°ë”ì— 'ADEX 2025 ì¤€ë¹„ íšŒì˜'ë¼ëŠ” ì´ë¦„ì˜ í•˜ë£¨ ì¢…ì¼ ì¼ì •ì„ ë“±ë¡í•´ì¤˜."
    # query = "ì˜¤ëŠ˜ ì„œìš¸ ë‚ ì”¨ ì•Œë ¤ì¤˜" # Simpler query

    print(f"\n\n===== Processing Query: \"{query}\" =====")
    verified_bp = phase1_generator.generate_verified_blueprint(query, max_attempts=3)

    if verified_bp:
        print("\nğŸ‰ Successfully generated Verified Blueprint! ğŸ‰")
        print("\nFinal Blueprint:")
        print(verified_bp.blueprint.model_dump_json(indent=2))
        print("\nValidation Result:")
        print(verified_bp.validation_result.model_dump_json(indent=2))
        print("\nLLM Review History:")
        for review_item in verified_bp.llm_review_history:
            print(review_item.model_dump_json(indent=2))
        print(f"Generated in {verified_bp.generation_attempts} attempt(s).")
    else:
        print("\nâŒ Failed to generate a verified blueprint for the query.")

    print("\n\n===== Phase 1 Generation Example Finished. =====")
