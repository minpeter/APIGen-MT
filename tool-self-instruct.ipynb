{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "37fa6e8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No existing query file found (diverse_queries_with_scores.json). Starting fresh.\n",
      "Starting Generation Process.\n",
      "Initial query objects loaded: 0\n",
      "Targeting 10 new queries per turn for 3 turns.\n",
      "Overall target: 30 query objects.\n",
      "Using Similarity Threshold: 0.7\n",
      "------------------------------\n",
      "\n",
      "=== Turn 1/3 ===\n",
      "\n",
      "--- Turn 1 | Attempt 1/5 ---\n",
      "Current total query objects: 0\n",
      "Goal for this turn: 0/10 new queries\n",
      "--- Calling LLM (deepseek-r1) to generate ~15 queries ---\n",
      "--- LLM Response Received ---\n",
      "--- Lines after removing <think> blocks: 16 ---\n",
      "--- Parsed 15 potentially valid candidate queries ---\n",
      "--- Filtering 15 candidates for diversity against 0 existing queries ---\n",
      "--- Accepted 15 new diverse query objects this round ---\n",
      "Accepted 10 new diverse query objects in this attempt.\n",
      "--- Turn 1 goal reached (10 new queries added). ---\n",
      "Saved 10 query objects to diverse_queries_with_scores.json\n",
      "--- End of Turn 1. Total query objects now: 10. Added this turn: 10. ---\n",
      "\n",
      "=== Turn 2/3 ===\n",
      "\n",
      "--- Turn 2 | Attempt 1/5 ---\n",
      "Current total query objects: 10\n",
      "Goal for this turn: 0/10 new queries\n",
      "--- Calling LLM (deepseek-r1) to generate ~15 queries ---\n",
      "--- LLM Response Received ---\n",
      "--- Lines after removing <think> blocks: 17 ---\n",
      "Filtered out invalid line: 'Check for any travel advisories in Rome due to weather.'\n",
      "--- Parsed 14 potentially valid candidate queries ---\n",
      "--- Filtering 14 candidates for diversity against 10 existing queries ---\n",
      "--- Accepted 14 new diverse query objects this round ---\n",
      "Accepted 10 new diverse query objects in this attempt.\n",
      "--- Turn 2 goal reached (10 new queries added). ---\n",
      "Saved 20 query objects to diverse_queries_with_scores.json\n",
      "--- End of Turn 2. Total query objects now: 20. Added this turn: 10. ---\n",
      "\n",
      "=== Turn 3/3 ===\n",
      "\n",
      "--- Turn 3 | Attempt 1/5 ---\n",
      "Current total query objects: 20\n",
      "Goal for this turn: 0/10 new queries\n",
      "--- Calling LLM (deepseek-r1) to generate ~15 queries ---\n",
      "--- LLM Response Received ---\n",
      "--- Lines after removing <think> blocks: 16 ---\n",
      "--- Parsed 14 potentially valid candidate queries ---\n",
      "--- Filtering 14 candidates for diversity against 20 existing queries ---\n",
      "--- Accepted 14 new diverse query objects this round ---\n",
      "Accepted 10 new diverse query objects in this attempt.\n",
      "--- Turn 3 goal reached (10 new queries added). ---\n",
      "Saved 30 query objects to diverse_queries_with_scores.json\n",
      "--- End of Turn 3. Total query objects now: 30. Added this turn: 10. ---\n",
      "------------------------------\n",
      "Generation process completed after 3 turns.\n",
      "Total query objects generated or loaded in this session: 30\n",
      "Total new query objects added in this session: 30\n",
      "Final results saved to diverse_queries_with_scores.json\n",
      "\n",
      "Final list of diverse query objects (showing last added):\n",
      "* 1. q: \"What's the current temperature in Tokyo using Fahrenheit?\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 2. q: \"Show headlines about renewable energy innovations.\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 3. q: \"Where am I located right now?\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 4. q: \"Check if it's snowing in Toronto today.\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 5. q: \"Get latest updates on artificial intelligence research from Nature journal.\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 6. q: \"How's the weather looking in Dubai this afternoon?\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 7. q: \"Find news articles about Mars rover discoveries.\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 8. q: \"What's the temperature here in Celsius?\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 9. q: \"Can you check the weather for my current location?\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 10. q: \"Fetch recent business news from Bloomberg.\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 11. q: \"Will it rain in San Francisco tomorrow?\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 12. q: \"How humid is Bangkok right now in Fahrenheit?\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 13. q: \"Show me the latest headlines about quantum computing.\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 14. q: \"What's the temperature in my current location using Kelvin?\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 15. q: \"Find news on the upcoming Olympics from BBC.\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 16. q: \"Is there a heatwave predicted for Madrid this week?\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 17. q: \"Get me updates on cybersecurity breaches this month.\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 18. q: \"What's the wind speed like in Chicago today?\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 19. q: \"Any breaking news on climate change policies?\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 20. q: \"Can you check the weather in Reykjavik for hiking conditions?\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 21. q: \"What's the humidity level in Bangkok right now?\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 22. q: \"Show me the latest news on artificial intelligence from MIT Tech Review.\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 23. q: \"Will it rain in London tomorrow?\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 24. q: \"Find updates on the ongoing peace talks in the Middle East.\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 25. q: \"What's the temperature in New York in Fahrenheit?\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 26. q: \"Get me the top business news from Bloomberg.\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 27. q: \"Is there any news about the Mars rover mission?\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 28. q: \"Check the weather in Tokyo for the next three days.\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 29. q: \"What are the headlines on climate change today?\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "* 30. q: \"How cold is it in Moscow right now?\" (MaxS: 0.000, AvgS: 0.000, TopSim: {})\n",
      "\n",
      "--- Placeholder for Step 2: Generating Full Blueprints ---\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import os\n",
    "from typing import List, Dict, Any, Optional, Annotated, Callable, Tuple\n",
    "from difflib import SequenceMatcher # Option 1 for basic similarity\n",
    "import heapq # For finding top N similar items efficiently\n",
    "\n",
    "# Option 2: Install rouge-score for Alpaca-like filtering: pip install rouge-score\n",
    "try:\n",
    "    from rouge_score import rouge_scorer\n",
    "    ROUGE_AVAILABLE = True\n",
    "except ImportError:\n",
    "    ROUGE_AVAILABLE = False\n",
    "    print(\"Warning: rouge-score library not found. Falling back to basic similarity filtering.\")\n",
    "    print(\"Install it via 'pip install rouge-score' for better diversity filtering.\")\n",
    "\n",
    "# --- Ensure openai library is installed ---\n",
    "try:\n",
    "    from openai import OpenAI, APIError\n",
    "except ImportError:\n",
    "    print(\"Error: openai library not found. Please install it using 'pip install openai'\")\n",
    "    exit()\n",
    "\n",
    "# --- Friendli AI Client Setup ---\n",
    "token = os.getenv(\"FRIENDLI_TOKEN\")\n",
    "if not token:\n",
    "    print(\"Error: FRIENDLI_TOKEN environment variable not set.\")\n",
    "    print(\"Please set the environment variable or replace '<YOUR_FRIENDLI_TOKEN>' in the code.\")\n",
    "    token = \"<YOUR_FRIENDLI_TOKEN>\" # Placeholder\n",
    "\n",
    "if token == \"<YOUR_FRIENDLI_TOKEN>\":\n",
    "     print(\"Warning: Using placeholder Friendli token. LLM calls will likely fail.\")\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url = \"https://api.friendli.ai/serverless/v1\",\n",
    "    api_key = token\n",
    ")\n",
    "LLM_MODEL_NAME = \"deepseek-r1\" # Specify the model\n",
    "\n",
    "# --- Tool Definitions and Schema Generation ---\n",
    "# Assuming function_schema.py exists and works as expected\n",
    "try:\n",
    "    from function_schema import get_function_schema\n",
    "except ImportError:\n",
    "    print(\"Error: function_schema.py not found. Please ensure it's in the same directory.\")\n",
    "    # Define dummy function if not found, to allow script to run partially\n",
    "    def get_function_schema(func):\n",
    "        return {\"name\": func.__name__, \"description\": func.__doc__, \"parameters\": {}}\n",
    "\n",
    "\n",
    "def get_weather(\n",
    "    city: Annotated[str, \"The city to get the weather for\"],\n",
    "    unit: Annotated[Optional[str], \"The unit to return the temperature in\"] = \"celcius\",\n",
    ") -> str:\n",
    "    \"\"\"Returns the weather for the given city.\"\"\"\n",
    "    return f\"Weather for {city} is 20°C\"\n",
    "\n",
    "def get_news(\n",
    "    topic: Annotated[str, \"The topic to get news for\"],\n",
    "    source: Annotated[Optional[str], \"The source to get news from\"] = None,\n",
    ") -> str:\n",
    "    \"\"\"Returns the news for the given topic.\"\"\"\n",
    "    return f\"News for {topic} from {source if source else 'all sources'}\"\n",
    "\n",
    "def get_current_location() -> str:\n",
    "    \"\"\"Returns the current location of the user.\"\"\"\n",
    "    # Example: Use context if available, otherwise a default\n",
    "    # Current time is Sunday, May 4, 2025 at 7:49 PM KST.\n",
    "    # Remember the current location is Seoul, Seoul, South Korea.\n",
    "    return \"Current location is Seoul, South Korea\"\n",
    "\n",
    "tools = [\n",
    "    get_weather,\n",
    "    get_news,\n",
    "    get_current_location,\n",
    "]\n",
    "\n",
    "tool_schemas = [get_function_schema(tool) for tool in tools]\n",
    "tool_schemas_json = json.dumps(tool_schemas, indent=2)\n",
    "\n",
    "\n",
    "# --- Configuration ---\n",
    "QUERIES_FILENAME = \"diverse_queries_with_scores.json\"\n",
    "NUM_GENERATION_TURNS = 3\n",
    "QUERIES_TO_GENERATE_PER_TURN = 10\n",
    "REQUEST_BATCH_SIZE_PER_TURN = 15 # Ask for slightly more than needed per turn\n",
    "MAX_ATTEMPTS_PER_TURN = 5\n",
    "SIMILARITY_THRESHOLD = 0.7\n",
    "TOP_N_SIMILAR = 5\n",
    "\n",
    "# --- Helper Functions ---\n",
    "\n",
    "def load_queries_with_scores(filename: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Loads previously generated query objects from a JSON file.\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        try:\n",
    "            with open(filename, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list) and all(isinstance(item, dict) and 'q' in item for item in data):\n",
    "                    print(f\"Loaded {len(data)} query objects from {filename}\")\n",
    "                    return data\n",
    "                else:\n",
    "                    print(f\"Warning: Invalid format found in {filename}. Starting fresh.\")\n",
    "                    return []\n",
    "        except (json.JSONDecodeError, IOError) as e:\n",
    "            print(f\"Error loading {filename}: {e}. Starting fresh.\")\n",
    "            return []\n",
    "    else:\n",
    "        print(f\"No existing query file found ({filename}). Starting fresh.\")\n",
    "        return []\n",
    "\n",
    "def save_queries_with_scores(query_objects: List[Dict[str, Any]], filename: str):\n",
    "    \"\"\"Saves the list of query objects to a JSON file.\"\"\"\n",
    "    try:\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(query_objects, f, indent=4, ensure_ascii=False)\n",
    "        print(f\"Saved {len(query_objects)} query objects to {filename}\")\n",
    "    except IOError as e:\n",
    "        print(f\"Error saving queries to {filename}: {e}\")\n",
    "\n",
    "def is_valid_query_line(q_text: str) -> bool:\n",
    "    \"\"\"\n",
    "    Checks if a single line looks like a valid user query, filtering out metadata/reasoning patterns.\n",
    "    This is applied *after* removing <think> blocks.\n",
    "    \"\"\"\n",
    "    q_text = q_text.strip()\n",
    "    if not q_text:\n",
    "        return False\n",
    "    # Filter out common non-query patterns often seen outside <think> blocks\n",
    "    if q_text.startswith((\"- \", \"* \", \"Okay,\", \"First,\", \"Next,\", \"Now,\", \"Let me\", \"Wait,\", \"Also,\", \"###\", \"//\", \"```\", \"Queries\", \"That's\", \"This should\", \"Avoid\", \"Check for\")):\n",
    "        return False\n",
    "    if q_text.endswith(\":\") or \"→\" in q_text: # Filter out lines describing steps\n",
    "        return False\n",
    "    if re.match(r\"^\\d+\\.\", q_text): # Filter out numbered lists\n",
    "         return False\n",
    "    # Filter out lines that are just punctuation or single words (likely errors)\n",
    "    if len(q_text.split()) <= 1 and not re.search(r'[a-zA-Z]', q_text):\n",
    "         return False\n",
    "    # Filter out lines that look like file paths or code snippets\n",
    "    if \"/\" in q_text and \".\" in q_text and \" \" not in q_text: # Basic check for path-like strings\n",
    "         return False\n",
    "    # Add more specific rules if needed based on observed LLM outputs\n",
    "    return True\n",
    "\n",
    "def remove_think_blocks(text: str) -> str:\n",
    "    \"\"\"Removes <think>...</think> blocks from the text using regex.\"\"\"\n",
    "    # Non-greedy match, case-insensitive, works across newlines\n",
    "    return re.sub(r\"<think>.*?</think>\", \"\", text, flags=re.DOTALL | re.IGNORECASE)\n",
    "\n",
    "# --- Step 1: Generate Diverse User Queries ('q') ---\n",
    "\n",
    "def generate_candidate_qs_with_llm(\n",
    "    tool_schemas_str: str,\n",
    "    num_to_generate: int,\n",
    "    existing_qs_list: List[str],\n",
    "    llm_client: OpenAI\n",
    ") -> List[str]:\n",
    "    \"\"\"Generates candidate 'q' strings using the LLM, removing think blocks.\"\"\"\n",
    "\n",
    "    # Strengthened system prompt\n",
    "    system_prompt = f\"\"\"Your ONLY task is to generate realistic, diverse user queries or requests ('q') suitable for an AI assistant with access to specific tools. These queries should be answerable using the provided tools. Vary the complexity, phrasing (questions, commands), and the tools potentially required.\n",
    "\n",
    "**CRITICAL INSTRUCTIONS:**\n",
    "1.  Output ONLY the raw user queries.\n",
    "2.  Each query MUST be on a new line.\n",
    "3.  **ABSOLUTELY DO NOT** include:\n",
    "    * Explanations, comments, or justifications.\n",
    "    * Thinking processes, reasoning steps (including anything like `<think>...</think>`).\n",
    "    * Numbered lists, bullet points, or any formatting other than one query per line.\n",
    "    * Any text before the first query or after the last query.\n",
    "\"\"\"\n",
    "\n",
    "    examples_prompt = \"\"\n",
    "    if existing_qs_list:\n",
    "        sample_existing = random.sample(existing_qs_list, min(len(existing_qs_list), 5))\n",
    "        examples_prompt = \"Critically, avoid generating queries too similar to these examples:\\n- \" + \"\\n- \".join(sample_existing) + \"\\n\\n\"\n",
    "\n",
    "    user_prompt = f\"\"\"Based on the following available tools:\n",
    "```json\n",
    "{tool_schemas_str}\n",
    "```\n",
    "\n",
    "Generate exactly {num_to_generate} diverse user queries ('q'). Remember to vary the required tools, complexity, and phrasing. {examples_prompt}Output ONLY the queries, one per line:\"\"\"\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt}\n",
    "    ]\n",
    "\n",
    "    try:\n",
    "        print(f\"--- Calling LLM ({LLM_MODEL_NAME}) to generate ~{num_to_generate} queries ---\")\n",
    "        completion = llm_client.chat.completions.create(\n",
    "            model=LLM_MODEL_NAME,\n",
    "            messages=messages,\n",
    "            temperature=0.8, # Slightly higher temp for more creative queries\n",
    "            max_tokens=2048 # Increased max_tokens\n",
    "        )\n",
    "        raw_llm_output = completion.choices[0].message.content\n",
    "        print(\"--- LLM Response Received ---\")\n",
    "    except APIError as e:\n",
    "        print(f\"LLM API Error: {e}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An unexpected error occurred during LLM call: {e}\")\n",
    "        return []\n",
    "\n",
    "    # 1. Remove <think> blocks first\n",
    "    cleaned_output = remove_think_blocks(raw_llm_output)\n",
    "\n",
    "    # 2. Split into lines and apply line-level validity filter\n",
    "    candidate_qs = []\n",
    "    raw_lines = cleaned_output.split('\\n')\n",
    "    print(f\"--- Lines after removing <think> blocks: {len(raw_lines)} ---\")\n",
    "    for line in raw_lines:\n",
    "        clean_line = line.strip()\n",
    "        if is_valid_query_line(clean_line): # Apply line filter here\n",
    "            candidate_qs.append(clean_line)\n",
    "        elif clean_line:\n",
    "             print(f\"Filtered out invalid line: '{clean_line}'\") # Log filtered lines\n",
    "\n",
    "    print(f\"--- Parsed {len(candidate_qs)} potentially valid candidate queries ---\")\n",
    "    return candidate_qs\n",
    "\n",
    "\n",
    "def calculate_similarity(q1: str, q2: str, scorer=None) -> float:\n",
    "    \"\"\"Calculates similarity score between two queries.\"\"\"\n",
    "    # Simple case: identical strings\n",
    "    if q1 == q2: return 1.0\n",
    "    # Avoid calculating similarity for very short strings (likely noise)\n",
    "    if len(q1) < 5 or len(q2) < 5: return 0.0\n",
    "\n",
    "    q1_lower = q1.lower()\n",
    "    q2_lower = q2.lower()\n",
    "    if scorer: # Use ROUGE if available\n",
    "        try:\n",
    "            # Ensure tokens are generated before scoring\n",
    "            q1_tokens = scorer._tokenizer.tokenize(q1)\n",
    "            q2_tokens = scorer._tokenizer.tokenize(q2)\n",
    "            if not q1_tokens or not q2_tokens: return 0.0 # Handle empty token lists\n",
    "            rouge_result = scorer._score_lcs(q1_tokens, q2_tokens)\n",
    "            return rouge_result.fmeasure if rouge_result else 0.0\n",
    "        except Exception as e:\n",
    "            # print(f\"Warning: ROUGE error between '{q1[:20]}...' and '{q2[:20]}...': {e}\") # Optional debug\n",
    "            return 0.0 # Error during ROUGE calculation\n",
    "    else: # Fallback to SequenceMatcher\n",
    "        return SequenceMatcher(None, q1_lower, q2_lower).ratio()\n",
    "\n",
    "\n",
    "def filter_and_score_qs(\n",
    "    candidate_qs: List[str],\n",
    "    existing_query_objects: List[Dict[str, Any]], # Pass the full objects\n",
    "    similarity_threshold: float = 0.7,\n",
    "    top_n_similar: int = 5\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"Filters candidate queries based on similarity and calculates scores.\"\"\"\n",
    "    newly_accepted_query_objects = []\n",
    "    existing_qs_list = [obj['q'] for obj in existing_query_objects]\n",
    "    # Comparison pool includes existing + newly accepted in this batch\n",
    "    all_qs_strings_for_comparison = list(existing_qs_list)\n",
    "\n",
    "    if not candidate_qs:\n",
    "        return []\n",
    "\n",
    "    scorer = None\n",
    "    if ROUGE_AVAILABLE:\n",
    "        try:\n",
    "            scorer = rouge_scorer.RougeScorer([\"rougeL\"], use_stemmer=False)\n",
    "        except Exception as e:\n",
    "            print(f\"Error initializing RougeScorer: {e}. Falling back.\")\n",
    "            scorer = None\n",
    "\n",
    "    print(f\"--- Filtering {len(candidate_qs)} candidates for diversity against {len(all_qs_strings_for_comparison)} existing queries ---\")\n",
    "    for q_new in candidate_qs:\n",
    "        q_new_lower = q_new.lower()\n",
    "\n",
    "        # Check exact duplicates against all known query strings\n",
    "        is_exact_duplicate = any(q_new_lower == q_old.lower() for q_old in all_qs_strings_for_comparison)\n",
    "        if is_exact_duplicate:\n",
    "            continue\n",
    "\n",
    "        # Calculate similarities against all existing query strings\n",
    "        similarities = []\n",
    "        if all_qs_strings_for_comparison:\n",
    "            for q_old in all_qs_strings_for_comparison:\n",
    "                score = calculate_similarity(q_new, q_old, scorer)\n",
    "                # Only store meaningful similarities to avoid clutter\n",
    "                if score > 0.1: # Threshold to store similarity\n",
    "                    similarities.append((score, q_old))\n",
    "\n",
    "        max_similarity = 0.0\n",
    "        avg_similarity = 0.0\n",
    "        most_similar_dict = {}\n",
    "        is_too_similar = False\n",
    "\n",
    "        if similarities:\n",
    "            # Calculate max and average only on stored similarities\n",
    "            scores_only = [s[0] for s in similarities]\n",
    "            max_similarity = max(scores_only) if scores_only else 0.0\n",
    "            avg_similarity = sum(scores_only) / len(scores_only) if scores_only else 0.0\n",
    "\n",
    "            # Find top N most similar using heapq from calculated similarities\n",
    "            top_n = heapq.nlargest(min(top_n_similar, len(similarities)), similarities, key=lambda item: item[0])\n",
    "            # Store with rounded scores for cleaner output\n",
    "            most_similar_dict = {q: round(score, 4) for score, q in top_n}\n",
    "\n",
    "            is_too_similar = max_similarity > similarity_threshold\n",
    "\n",
    "        if not is_too_similar:\n",
    "            new_obj = {\n",
    "                \"q\": q_new,\n",
    "                \"max_similarity_score\": round(max_similarity, 4),\n",
    "                \"avg_similarity_score\": round(avg_similarity, 4),\n",
    "                \"most_similar_queries\": most_similar_dict\n",
    "            }\n",
    "            newly_accepted_query_objects.append(new_obj)\n",
    "            # Add the new query string to the comparison pool immediately\n",
    "            all_qs_strings_for_comparison.append(q_new)\n",
    "\n",
    "    print(f\"--- Accepted {len(newly_accepted_query_objects)} new diverse query objects this round ---\")\n",
    "    return newly_accepted_query_objects\n",
    "\n",
    "\n",
    "# --- Main Execution Logic ---\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    accepted_query_objects = load_queries_with_scores(QUERIES_FILENAME)\n",
    "    initial_query_count = len(accepted_query_objects)\n",
    "    overall_target = initial_query_count + (NUM_GENERATION_TURNS * QUERIES_TO_GENERATE_PER_TURN)\n",
    "\n",
    "    print(f\"Starting Generation Process.\")\n",
    "    print(f\"Initial query objects loaded: {initial_query_count}\")\n",
    "    print(f\"Targeting {QUERIES_TO_GENERATE_PER_TURN} new queries per turn for {NUM_GENERATION_TURNS} turns.\")\n",
    "    print(f\"Overall target: {overall_target} query objects.\")\n",
    "    print(f\"Using Similarity Threshold: {SIMILARITY_THRESHOLD}\")\n",
    "    print(\"-\" * 30)\n",
    "\n",
    "    total_added_this_session = 0\n",
    "\n",
    "    for turn in range(1, NUM_GENERATION_TURNS + 1):\n",
    "        print(f\"\\n=== Turn {turn}/{NUM_GENERATION_TURNS} ===\")\n",
    "        target_for_this_turn = QUERIES_TO_GENERATE_PER_TURN\n",
    "        added_in_this_turn = 0\n",
    "        attempts_this_turn = 0\n",
    "\n",
    "        # Extract current query strings for prompting\n",
    "        current_qs_list = [obj['q'] for obj in accepted_query_objects]\n",
    "\n",
    "        while added_in_this_turn < target_for_this_turn and attempts_this_turn < MAX_ATTEMPTS_PER_TURN:\n",
    "            attempts_this_turn += 1\n",
    "            print(f\"\\n--- Turn {turn} | Attempt {attempts_this_turn}/{MAX_ATTEMPTS_PER_TURN} ---\")\n",
    "            print(f\"Current total query objects: {len(accepted_query_objects)}\")\n",
    "            print(f\"Goal for this turn: {added_in_this_turn}/{target_for_this_turn} new queries\")\n",
    "\n",
    "            num_needed_for_turn = target_for_this_turn - added_in_this_turn\n",
    "            num_to_generate_this_attempt = min(REQUEST_BATCH_SIZE_PER_TURN, num_needed_for_turn + 5)\n",
    "\n",
    "            # Generate candidate query strings\n",
    "            candidate_qs_strings = generate_candidate_qs_with_llm(\n",
    "                tool_schemas_json,\n",
    "                num_to_generate=num_to_generate_this_attempt,\n",
    "                existing_qs_list=current_qs_list, # Pass only strings for prompt\n",
    "                llm_client=client\n",
    "            )\n",
    "\n",
    "            if not candidate_qs_strings:\n",
    "                print(\"LLM did not return any valid candidate queries or an error occurred. Retrying after delay...\")\n",
    "                time.sleep(5)\n",
    "                continue\n",
    "\n",
    "            # Filter candidates and create new query objects with scores\n",
    "            # Pass the full list of objects for accurate similarity calculation context\n",
    "            new_query_objects = filter_and_score_qs(\n",
    "                candidate_qs_strings,\n",
    "                accepted_query_objects, # Compare against all accepted objects so far\n",
    "                similarity_threshold=SIMILARITY_THRESHOLD,\n",
    "                top_n_similar=TOP_N_SIMILAR\n",
    "            )\n",
    "\n",
    "            # Add the newly accepted query objects\n",
    "            added_now = 0\n",
    "            for obj in new_query_objects:\n",
    "                if added_in_this_turn < target_for_this_turn:\n",
    "                    accepted_query_objects.append(obj)\n",
    "                    # Update the list used ONLY for prompting in the next attempt/turn\n",
    "                    current_qs_list.append(obj['q'])\n",
    "                    added_in_this_turn += 1\n",
    "                    added_now += 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            print(f\"Accepted {added_now} new diverse query objects in this attempt.\")\n",
    "\n",
    "            if added_now == 0 and attempts_this_turn > 1:\n",
    "                print(\"Warning: No new diverse queries accepted in this attempt.\")\n",
    "\n",
    "            if added_in_this_turn >= target_for_this_turn:\n",
    "                print(f\"--- Turn {turn} goal reached ({added_in_this_turn} new queries added). ---\")\n",
    "                break\n",
    "\n",
    "            time.sleep(1) # Small delay between attempts\n",
    "\n",
    "        # Save after each turn\n",
    "        total_added_this_session += added_in_this_turn\n",
    "        save_queries_with_scores(accepted_query_objects, QUERIES_FILENAME)\n",
    "        print(f\"--- End of Turn {turn}. Total query objects now: {len(accepted_query_objects)}. Added this turn: {added_in_this_turn}. ---\")\n",
    "\n",
    "\n",
    "    print(\"-\" * 30)\n",
    "    print(f\"Generation process completed after {NUM_GENERATION_TURNS} turns.\")\n",
    "    print(f\"Total query objects generated or loaded in this session: {len(accepted_query_objects)}\")\n",
    "    print(f\"Total new query objects added in this session: {total_added_this_session}\")\n",
    "    print(f\"Final results saved to {QUERIES_FILENAME}\")\n",
    "\n",
    "    print(\"\\nFinal list of diverse query objects (showing last added):\")\n",
    "    start_index = max(0, len(accepted_query_objects) - total_added_this_session)\n",
    "    for i, obj in enumerate(accepted_query_objects):\n",
    "         marker = \"*\" if i >= start_index else \" \"\n",
    "         similar_dict = obj.get('most_similar_queries', {})\n",
    "         # Format similar queries for concise printing\n",
    "         similar_items = [f\"'{q[:30]}...':{s:.2f}\" for q, s in similar_dict.items()]\n",
    "         similar_str = \", \".join(similar_items) if similar_items else \"{}\"\n",
    "\n",
    "         print(f\"{marker} {i+1}. q: \\\"{obj['q']}\\\" (MaxS: {obj.get('max_similarity_score', 0):.3f}, AvgS: {obj.get('avg_similarity_score', 0):.3f}, TopSim: {similar_str})\")\n",
    "\n",
    "\n",
    "    # --- Placeholder for Step 2 (remains the same concept) ---\n",
    "    print(\"\\n--- Placeholder for Step 2: Generating Full Blueprints ---\")\n",
    "    # Iterate through accepted_query_objects, take 'q', generate full Blueprint\n",
    "    # ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34ba5cc",
   "metadata": {},
   "source": [
    "\n",
    "**설명:**\n",
    "\n",
    "1.  **도구 및 스키마:** 기존 코드를 사용하여 도구(`tools`)와 해당 JSON 스키마(`tool_schemas`, `tool_schemas_json`)를 정의합니다.\n",
    "2.  **LLM 호출 시뮬레이션:** `dummy_llm_q_generation` 함수는 실제 LLM API 호출을 대체합니다. 실제 구현에서는 이 부분을 사용 중인 LLM 라이브러리(예: OpenAI, Anthropic, Hugging Face 등)의 API 호출 코드로 바꿔야 합니다. 이 더미 함수는 미리 정의된 질문 목록에서 무작위로 선택하고 약간의 중복을 추가하여 필터링 로직을 테스트할 수 있도록 합니다.\n",
    "3.  **`generate_candidate_qs` 함수:**\n",
    "    * LLM에게 다양한 `q`를 생성하도록 요청하는 시스템 및 사용자 프롬프트를 구성합니다.\n",
    "    * `tool_schemas_json`을 컨텍스트로 제공합니다.\n",
    "    * (선택 사항) 이미 생성된 `q` 샘플을 프롬프트에 포함하여 LLM이 유사한 것을 피하도록 유도합니다.\n",
    "    * LLM 응답(한 줄에 하나의 쿼리로 가정)을 파싱하여 후보 `q` 목록을 반환합니다.\n",
    "4.  **`filter_qs_for_diversity` 함수:**\n",
    "    * `rouge-score` 라이브러리가 있으면 ROUGE-L 점수를 사용하여 Alpaca와 유사하게 유사성을 계산합니다.\n",
    "    * 라이브러리가 없으면 Python 내장 `difflib.SequenceMatcher`를 사용하여 기본적인 문자열 유사성 비율을 계산합니다 (덜 정교함).\n",
    "    * 각 후보 `q`를 이전에 수락된 모든 `q`와 비교합니다.\n",
    "    * 유사성 점수가 설정된 임계값(`similarity_threshold`)보다 낮으면 해당 `q`를 수락합니다.\n",
    "    * 정확히 동일한 쿼리(대소문자 무시)는 먼저 필터링합니다.\n",
    "5.  **메인 실행 로직:**\n",
    "    * 원하는 쿼리 수(`NUM_QUERIES_DESIRED`), LLM 호출당 생성할 후보 수(`REQUEST_BATCH_SIZE`), 유사성 임계값 등을 설정합니다.\n",
    "    * 루프를 돌면서 `generate_candidate_qs`로 후보를 생성하고 `filter_qs_for_diversity`로 필터링하여 `accepted_qs` 목록을 채웁니다.\n",
    "    * 무한 루프를 방지하기 위해 최대 시도 횟수(`MAX_ATTEMPTS`)를 설정합니다.\n",
    "    * 최종적으로 생성된 다양한 `q` 목록을 출력합니다.\n",
    "6.  **다음 단계:** 주석 처리된 부분처럼, 이 코드에서 생성된 `accepted_qs` 목록의 각 `q`를 원래의 `Blueprint` 생성 로직(예: `structuredOutput` 함수)에 입력으로 사용하여 `a_gt_steps`와 `o_gt`를 포함한 완전한 JSON 객체를 만들 수 있습니다.\n",
    "\n",
    "**실행 방법:**\n",
    "\n",
    "1.  `function_schema.py` 파일이 있는지 확인합니다.\n",
    "2.  (선택 사항) `pip install rouge-score` 를 실행하여 더 나은 필터링을 사용합니다.\n",
    "3.  `dummy_llm_q_generation` 함수를 실제 LLM API 호출 코드로 교체합니다.\n",
    "4.  스크립트를 실행합니다.\n",
    "\n",
    "이제 이 코드를 사용하여 주어진 도구 세트에 대해 다양한 사용자 질문(`q`)을 생성하고, 이를 바탕으로 최종 `Blueprint` 데이터를 구축할 수 있습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
